# 提示词效果评估工具使用指南

## 概述

本工具旨在评估不同提示词模板在行人重识别(ReID)任务中的效果。通过综合考虑**token消耗**和**信息程度**（准确性+详细性），帮助用户选择最优的提示词模板。

## 核心原理

评估算法基于以下流程：

1. **准备候选提示词**：给定若干不同的prompt模板
2. **计算token数**：统计每个提示词消耗的token数
3. **评估信息准确性**：使用CLIP计算描述与原图像的相似度
4. **评估信息详细性**：统计描述中出现的属性维度数量
5. **计算信息程度分数**：$I = w_a \times A + w_d \times D$（线性加权）
6. **计算综合得分**：$S = I / T$（信息密度），即单位token带来的有效信息量

## 工具文件说明

1. `src/eval_prompt_effectiveness.py` - 核心评估算法实现
2. `scripts/test_prompt_evaluation.py` - 测试脚本，用于演示评估流程

## 安装依赖

使用前请确保已安装以下依赖：

```bash
pip install transformers Pillow torch numpy tqdm
```

## 使用方法

### 1. 直接运行评估脚本

```bash
python src/eval_prompt_effectiveness.py --descriptions_file /path/to/descriptions.csv --images_dir /path/to/images --output_file /path/to/output.json
```

### 2. 命令行参数说明

| 参数名 | 类型 | 默认值 | 说明 |
|--------|------|--------|------|
| `--prompts_dir` | 字符串 | `./prompts` | 提示词模板目录 |
| `--descriptions_file` | 字符串 | （必需） | 已生成的描述文件（CSV格式） |
| `--images_dir` | 字符串 | `./data/market1501/bounding_box_test` | 图像目录 |
| `--output_file` | 字符串 | `./outputs/prompt_evaluation.json` | 评估结果输出文件 |
| `--w_a` | 浮点数 | `0.6` | 准确性权重 |
| `--w_d` | 浮点数 | `0.4` | 详细性权重 |

### 3. 测试评估流程

可以使用测试脚本快速体验评估流程：

```bash
python scripts/test_prompt_evaluation.py
```

该测试脚本会：
- 创建样本行人图像
- 生成不同详细程度的描述
- 运行评估算法（默认使用模拟结果）
- 分析并展示评估结果

## 输入数据格式

### 描述文件（CSV格式）

描述文件需要包含以下列：

```csv
image_id,prompt,caption,timestamp
```

- `image_id`: 图像的唯一标识符
- `prompt`: 用于生成描述的提示词
- `caption`: 模型生成的行人描述文本
- `timestamp`: 生成时间戳

**注意**：如果`caption`字段包含逗号，需要用引号包围整个字段。

## 输出结果格式

评估结果以JSON格式保存，包含以下主要信息：

```json
[
  {
    "prompt": "提示词文本",
    "num_samples": 样本数量,
    "average_scores": {
      "tokens": 平均token数,
      "accuracy": 平均准确性得分,
      "detail": 平均详细性得分,
      "info_score": 平均信息程度得分,
      "final_score": 平均综合得分
    },
    "individual_results": [
      {
        "image_id": "图像ID",
        "caption": "描述文本",
        "scores": {
          "tokens": token数,
          "accuracy": 准确性得分,
          "detail": 详细性得分,
          "info_score": 信息程度得分,
          "final_score": 综合得分
        }
      }
    ]
  }
]
```

结果会按照`final_score`（综合得分）降序排列，得分最高的提示词排在前面。

## 详细功能说明

### 1. CLIP模型初始化

工具会自动加载预训练的CLIP模型（默认为`openai/clip-vit-base-patch32`），用于计算文本描述与图像的相似度。如果系统支持GPU，会自动使用GPU加速。

### 2. Token计数

使用GPT-2的tokenizer计算提示词的token数量。如果无法加载tokenizer，会自动降级使用基于空格的简单分词方法。

### 3. 准确性评估

使用CLIP模型计算文本描述与原始图像的余弦相似度，值范围为0~1，越高表示描述与图像越匹配。

### 4. 详细性评估

通过关键词匹配统计描述中提及的属性类别数量，包括：
- **性别**：男、女、man、woman等
- **上衣**：T恤、衬衫、夹克、上衣等
- **下装**：裤、裙、裤子、裙子等
- **鞋子**：鞋、鞋子、运动鞋等
- **包**：包、背包、手提包等

详细性得分 = 实际提及的属性类别数量 / 总属性类别数量

### 5. 综合得分计算

综合考虑token消耗和信息程度，计算单位token带来的有效信息量：

- 信息程度分数 $I = w_a \times A + w_d \times D$
  - $w_a$: 准确性权重（默认0.6）
  - $A$: 准确性得分
  - $w_d$: 详细性权重（默认0.4）
  - $D$: 详细性得分
- 综合得分 $S = I / T$
  - $T$: token数量

## 实践建议

1. **调整权重参数**：根据实际需求调整`--w_a`和`--w_d`参数，平衡准确性和详细性的重要性

2. **批量评估**：建议对每个提示词模板生成多个样本的描述，以获得更可靠的平均得分

3. **结果分析**：评估结果不仅可以用于选择最优提示词，还可以分析不同提示词的优缺点，指导提示词优化

4. **大规模评估**：在处理大量数据时，建议分批进行评估，避免内存不足

## 扩展功能

### 自定义属性类别

可以根据特定任务需求修改`evaluate_detail`函数中的`attributes`字典，调整要检测的属性类别和关键词：

```python
attributes = {
    "gender": ["男", "女", "man", "woman"],
    "top": ["T恤", "衬衫", "夹克", "上衣"],
    # 可以添加更多自定义属性类别
}
```

### 集成标注属性

如果有标注的行人属性数据，可以修改`evaluate_accuracy`函数，使用属性匹配准确率替代CLIP相似度：

```python
def evaluate_accuracy_with_annotations(description, annotations):
    # 计算描述与标注属性的匹配程度
    # ...
```

## 示例输出

```
[INFO] 提示词评估结果总结：

排名 1:
提示词: 请详细描述图像中的行人，包括性别、衣着颜色、配饰。...
样本数: 5
平均token数: 15.00
平均准确性: 0.8500
平均详细性: 0.8000
平均信息分数: 0.8300
平均综合得分: 0.055333

排名 2:
提示词: 简要描述图中的人。...
样本数: 5
平均token数: 8.00
平均准确性: 0.5000
平均详细性: 0.2000
平均信息分数: 0.3800
平均综合得分: 0.047500
```

在这个示例中，第一个提示词虽然消耗更多token，但提供了更高的准确性与详细性，最终信息密度更高。

## 注意事项

1. **图像加载**：工具假设图像文件名与`image_id`对应，如果文件命名不同，可能需要修改图像加载逻辑

2. **中文支持**：处理中文文本时，确保使用正确的字符编码（UTF-8）

3. **性能考虑**：CLIP模型推理可能需要一定的计算资源，在处理大量图像时建议使用GPU

4. **评估偏差**：评估结果可能受到样本数量、图像质量等因素的影响，建议使用足够多的样本来获得可靠结果